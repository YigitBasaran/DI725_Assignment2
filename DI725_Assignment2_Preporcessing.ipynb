{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74315,
     "status": "ok",
     "timestamp": 1744372808805,
     "user": {
      "displayName": "ali yiƒüit ba≈üaran",
      "userId": "07561520228647496516"
     },
     "user_tz": -180
    },
    "id": "2KoQbyJJhdby",
    "outputId": "35e963ca-131a-4a9a-edb0-a268bb872885"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6689,
     "status": "ok",
     "timestamp": 1744372815500,
     "user": {
      "displayName": "ali yiƒüit ba≈üaran",
      "userId": "07561520228647496516"
     },
     "user_tz": -180
    },
    "id": "iUyljFIeIZPW",
    "outputId": "dc4f30f9-4bea-49fe-b41a-b741cb9d871c"
   },
   "outputs": [],
   "source": [
    "# !pip install pycocotools\n",
    "# !pip install -U -q datasets transformers[torch] timm wandb torchmetrics matplotlib albumentations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "eqZdxHfZbCye"
   },
   "outputs": [],
   "source": [
    "# !pip install -q gdown\n",
    "# !gdown --id 1pJ3xfKtHiTdysX5G3dxqKTdGESOBYCxJ --output \"./AU-AIR.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# with zipfile.ZipFile(\"./AU-AIR.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"./AU-AIR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('./annotations/annotations.json') as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModelForObjectDetection, AutoImageProcessor\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torch.nn.functional import softmax\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mec4DpF1Lzzd"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8pTVflov19_"
   },
   "source": [
    "### Clean the keys of annotations from ':'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3721,
     "status": "ok",
     "timestamp": 1744373338958,
     "user": {
      "displayName": "ali yiƒüit ba≈üaran",
      "userId": "07561520228647496516"
     },
     "user_tz": -180
    },
    "id": "e3j1hBRHHOzt",
    "outputId": "6561f34f-ca17-4b3e-bafe-c5e4bf909b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned annotations saved to: ./annotations/annotations.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# File paths\n",
    "input_path = \"./annotations/annotations.json\"\n",
    "output_path = \"./annotations/annotations.json\"\n",
    "\n",
    "# Recursive function to clean dictionary keys\n",
    "def clean_keys(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k.rstrip(\":\"): clean_keys(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_keys(i) for i in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Load original data\n",
    "with open(input_path, \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Clean all keys\n",
    "cleaned_data = clean_keys(raw_data)\n",
    "\n",
    "# Save cleaned data\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(cleaned_data, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Cleaned annotations saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9RUUIOfMF_Y"
   },
   "source": [
    "### Convert the annotations to COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5065,
     "status": "ok",
     "timestamp": 1744373349456,
     "user": {
      "displayName": "ali yiƒüit ba≈üaran",
      "userId": "07561520228647496516"
     },
     "user_tz": -180
    },
    "id": "0VwHYJ-HD6eN",
    "outputId": "4a2bf0d5-4b30-4357-cfaa-6191ff82ddc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ COCO-format annotations saved to: ./annotations/auair_coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Step 1: Load cleaned annotation file\n",
    "with open(\"./annotations/annotations.json\", \"r\") as f:\n",
    "    auair_data = json.load(f)\n",
    "\n",
    "# Step 2: Convert to COCO format\n",
    "def convert_auair_to_coco(auair_data, output_path=\"auair_coco.json\"):\n",
    "    images = []\n",
    "    annotations = []\n",
    "\n",
    "    # Define COCO-style categories from class names\n",
    "    categories = [{\"id\": i, \"name\": name} for i, name in enumerate(auair_data[\"categories\"])]\n",
    "\n",
    "    ann_id = 1  # unique annotation ID\n",
    "    for img_id, entry in enumerate(auair_data[\"annotations\"], start=1):\n",
    "        image_name = entry[\"image_name\"]\n",
    "        width = entry[\"image_width\"]\n",
    "        height = entry[\"image_height\"]\n",
    "\n",
    "        images.append({\n",
    "            \"id\": img_id,\n",
    "            \"file_name\": image_name,\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "\n",
    "        for bbox in entry[\"bbox\"]:\n",
    "            x = bbox[\"left\"]\n",
    "            y = bbox[\"top\"]\n",
    "            w = bbox[\"width\"]\n",
    "            h = bbox[\"height\"]\n",
    "            category_id = bbox[\"class\"]\n",
    "\n",
    "            annotations.append({\n",
    "                \"id\": ann_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [x, y, w, h],       # COCO: [x, y, width, height]\n",
    "                \"area\": w * h,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            ann_id += 1\n",
    "\n",
    "    coco_data = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": categories\n",
    "    }\n",
    "\n",
    "    # Step 3: Save to COCO format JSON file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(coco_data, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ COCO-format annotations saved to: {output_path}\")\n",
    "\n",
    "# Step 4: Run the conversion\n",
    "convert_auair_to_coco(\n",
    "    auair_data,\n",
    "    output_path=\"./annotations/auair_coco.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1O-yMYuCvrr-"
   },
   "source": [
    "### Filter the COCO annotations such that images that are not annotated and/or annotated but no image file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129491,
     "status": "ok",
     "timestamp": 1744373484111,
     "user": {
      "displayName": "ali yiƒüit ba≈üaran",
      "userId": "07561520228647496516"
     },
     "user_tz": -180
    },
    "id": "iBswQnGxvr-v",
    "outputId": "cda17ea5-68c2-45bd-ed40-96dc37c8acbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total annotated images: 32823\n",
      "üìÇ Total actual image files: 32823\n",
      "\n",
      "‚ùå Missing image files (in annotations but not found on disk): 0\n",
      "üìù Unannotated image files (exist in folder but not in annotations): 0\n",
      "\n",
      "‚úÖ Filtered COCO annotations saved to: ./annotations/auair_coco_filtered.json\n",
      "‚û°Ô∏è Valid images remaining: 32823\n",
      "‚û°Ô∏è Valid annotations remaining: 132031\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# === PATHS ===\n",
    "image_folder = \"./AU-AIR/images\"\n",
    "annotation_file = \"./annotations/auair_coco.json\"\n",
    "filtered_annotation_path = \"./annotations/auair_coco_filtered.json\"\n",
    "\n",
    "# === LOAD ANNOTATION ===\n",
    "with open(annotation_file, \"r\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# === IMAGE NAME CHECKS ===\n",
    "annotated_image_names = set(img[\"file_name\"] for img in coco[\"images\"])\n",
    "image_files = set(os.listdir(image_folder))\n",
    "\n",
    "missing_images = annotated_image_names - image_files\n",
    "unannotated_images = image_files - annotated_image_names\n",
    "\n",
    "# === REPORT ===\n",
    "print(f\"‚úÖ Total annotated images: {len(annotated_image_names)}\")\n",
    "print(f\"üìÇ Total actual image files: {len(image_files)}\\n\")\n",
    "\n",
    "print(f\"‚ùå Missing image files (in annotations but not found on disk): {len(missing_images)}\")\n",
    "if missing_images:\n",
    "    print(\"Examples:\", sorted(list(missing_images))[:5])\n",
    "\n",
    "print(f\"üìù Unannotated image files (exist in folder but not in annotations): {len(unannotated_images)}\")\n",
    "if unannotated_images:\n",
    "    print(\"Examples:\", sorted(list(unannotated_images))[:5])\n",
    "\n",
    "# === FILTER ANNOTATIONS ===\n",
    "valid_images = [img for img in coco[\"images\"] if img[\"file_name\"] in image_files]\n",
    "valid_image_ids = set(img[\"id\"] for img in valid_images)\n",
    "valid_annotations = [ann for ann in coco[\"annotations\"] if ann[\"image_id\"] in valid_image_ids]\n",
    "\n",
    "filtered_coco = {\n",
    "    \"images\": valid_images,\n",
    "    \"annotations\": valid_annotations,\n",
    "    \"categories\": coco[\"categories\"]\n",
    "}\n",
    "\n",
    "# === SAVE CLEANED JSON ===\n",
    "with open(filtered_annotation_path, \"w\") as f:\n",
    "    json.dump(filtered_coco, f, indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ Filtered COCO annotations saved to: {filtered_annotation_path}\")\n",
    "print(f\"‚û°Ô∏è Valid images remaining: {len(valid_images)}\")\n",
    "print(f\"‚û°Ô∏è Valid annotations remaining: {len(valid_annotations)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Invalid bbox removed: id=108 bbox=[1892, 0, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=111 bbox=[874, 689, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=3453 bbox=[1022, 185, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=10683 bbox=[1563, 1080, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=24501 bbox=[677, 398, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=24502 bbox=[680, 389, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=25837 bbox=[559, 797, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=28448 bbox=[795, 699, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=28456 bbox=[1760, 290, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=30016 bbox=[691, 995, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=30017 bbox=[725, 932, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=30018 bbox=[696, 927, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=31110 bbox=[1205, 1080, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=31111 bbox=[1495, 1080, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=31113 bbox=[1685, 382, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=41260 bbox=[1640, 347, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=48670 bbox=[802, 98, 0, 4]\n",
      "‚ùå Invalid bbox removed: id=72018 bbox=[901, 599, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=77124 bbox=[1329, 557, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=77172 bbox=[1781, 1003, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=77419 bbox=[1485, 562, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=79381 bbox=[1438, 503, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=82214 bbox=[1651, 302, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=82266 bbox=[1646, 370, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=84156 bbox=[931, 114, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=84191 bbox=[585, 0, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=84257 bbox=[1672, 252, 0, 3]\n",
      "‚ùå Invalid bbox removed: id=85536 bbox=[965, 2, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=85767 bbox=[1327, 793, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=86081 bbox=[703, 350, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=86490 bbox=[859, 25, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=89249 bbox=[886, 766, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=91013 bbox=[653, 485, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=92458 bbox=[85, 215, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=95963 bbox=[1263, 520, 0, 4]\n",
      "‚ùå Invalid bbox removed: id=96833 bbox=[1061, 925, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=98157 bbox=[1300, 0, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=100491 bbox=[405, 22, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=100917 bbox=[862, 647, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=100918 bbox=[487, 996, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=104045 bbox=[1920, 1080, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=104046 bbox=[1102, 662, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=104050 bbox=[943, 188, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=104051 bbox=[902, 205, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=104052 bbox=[810, 106, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=104073 bbox=[673, 457, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=109124 bbox=[1666, 441, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=109920 bbox=[1441, 850, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=110580 bbox=[1798, 1009, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=115070 bbox=[1876, 1080, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=127925 bbox=[0, 1080, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=127926 bbox=[78, 288, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=127927 bbox=[128, 280, 0, 0]\n",
      "‚ùå Invalid bbox removed: id=130066 bbox=[862, 239, 0, 0]\n",
      "\n",
      "‚úÖ Cleaned annotations saved to: ./annotations/auair_coco_cleaned.json\n",
      "‚û°Ô∏è Total valid annotations: 131977\n",
      "üö´ Total invalid annotations removed: 54\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Input and output file paths\n",
    "input_path = \"./annotations/auair_coco_filtered.json\"\n",
    "output_path = \"./annotations/auair_coco_cleaned.json\"\n",
    "\n",
    "# Load COCO annotations\n",
    "with open(input_path, \"r\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "valid_annotations = []\n",
    "invalid_count = 0\n",
    "\n",
    "for ann in coco[\"annotations\"]:\n",
    "    x, y, w, h = ann[\"bbox\"]\n",
    "    if w > 0 and h > 0:\n",
    "        valid_annotations.append(ann)\n",
    "    else:\n",
    "        invalid_count += 1\n",
    "        print(f\"‚ùå Invalid bbox removed: id={ann['id']} bbox={ann['bbox']}\")\n",
    "\n",
    "# Keep the same image and category sections\n",
    "cleaned_coco = {\n",
    "    \"images\": coco[\"images\"],\n",
    "    \"annotations\": valid_annotations,\n",
    "    \"categories\": coco[\"categories\"]\n",
    "}\n",
    "\n",
    "# Save to new cleaned file\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(cleaned_coco, f, indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ Cleaned annotations saved to: {output_path}\")\n",
    "print(f\"‚û°Ô∏è Total valid annotations: {len(valid_annotations)}\")\n",
    "print(f\"üö´ Total invalid annotations removed: {invalid_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_22968\\1357501636.py:38: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(categories, rotation=45, ha=\"right\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AU-AIR class mapping (adjust if different)\n",
    "id2label = {\n",
    "    0: \"Human\",\n",
    "    1: \"Car\",\n",
    "    2: \"Truck\",\n",
    "    3: \"Van\",\n",
    "    4: \"Motorbike\",\n",
    "    5: \"Bicycle\",\n",
    "    6: \"Bus\",\n",
    "    7: \"Trailer\"\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Load cleaned annotation file\n",
    "with open(\"./annotations/auair_coco_cleaned.json\", \"r\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Count class occurrences\n",
    "class_counts = Counter()\n",
    "for ann in coco[\"annotations\"]:\n",
    "    class_counts[ann[\"category_id\"]] += 1\n",
    "\n",
    "# Prepare for plotting\n",
    "categories = [id2label[i] for i in class_counts.keys()]\n",
    "values = list(class_counts.values())\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars = ax.bar(categories, values, color=\"skyblue\")\n",
    "\n",
    "ax.set_xlabel(\"Categories\", fontsize=14)\n",
    "ax.set_ylabel(\"Number of Occurrences\", fontsize=14)\n",
    "ax.set_title(\"Object Frequency in AU-AIR Dataset\", fontsize=16)\n",
    "ax.set_xticklabels(categories, rotation=45, ha=\"right\")\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2.0, height, f\"{height}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ train split saved with 26258 images and 105274 annotations.\n",
      "‚úÖ val split saved with 3282 images and 13433 annotations.\n",
      "‚úÖ test split saved with 3283 images and 13270 annotations.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Load full COCO data\n",
    "with open(\"./annotations/auair_coco_cleaned.json\", \"r\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Get all image IDs\n",
    "all_images = coco[\"images\"]\n",
    "train_imgs, testval_imgs = train_test_split(all_images, test_size=0.2, random_state=42)\n",
    "val_imgs, test_imgs = train_test_split(testval_imgs, test_size=0.5, random_state=42)\n",
    "\n",
    "# Utility to get annotations for given image list\n",
    "def get_anns_for_imgs(images, all_anns):\n",
    "    img_ids = set(img[\"id\"] for img in images)\n",
    "    return [ann for ann in all_anns if ann[\"image_id\"] in img_ids]\n",
    "\n",
    "# Create new COCO-style splits\n",
    "def save_split(name, images, anns):\n",
    "    data = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": anns,\n",
    "        \"categories\": coco[\"categories\"]\n",
    "    }\n",
    "    with open(f\"./annotations/auair_coco_{name}.json\", \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"‚úÖ {name} split saved with {len(images)} images and {len(anns)} annotations.\")\n",
    "\n",
    "save_split(\"train\", train_imgs, get_anns_for_imgs(train_imgs, coco[\"annotations\"]))\n",
    "save_split(\"val\", val_imgs, get_anns_for_imgs(val_imgs, coco[\"annotations\"]))\n",
    "save_split(\"test\", test_imgs, get_anns_for_imgs(test_imgs, coco[\"annotations\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöó Number of images containing only 'Car': 11577\n",
      "‚úÖ Number of retained 'only-Car' images after downsampling: 2315\n",
      "\n",
      "üéØ Balanced train set saved to: ./annotations/auair_coco_train.json\n",
      "üñºÔ∏è Total remaining images: 16996\n",
      "‚úèÔ∏è Total remaining annotations: 80060\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Load the COCO-style annotation file\n",
    "with open(\"./annotations/auair_coco_train.json\", \"r\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Step 1: Map each image_id to the set of category_ids it contains\n",
    "image_categories = defaultdict(set)\n",
    "for ann in coco[\"annotations\"]:\n",
    "    image_categories[ann[\"image_id\"]].add(ann[\"category_id\"])\n",
    "\n",
    "# Step 2: Identify image_ids that contain only 'Car' (category_id = 1)\n",
    "only_car_ids = [img_id for img_id, cats in image_categories.items() if cats == {1}]\n",
    "print(f\"üöó Number of images containing only 'Car': {len(only_car_ids)}\")\n",
    "\n",
    "# Step 3: Select a fraction (e.g., 20%) to retain\n",
    "retain_ratio = 0.2\n",
    "retain_count = int(len(only_car_ids) * retain_ratio)\n",
    "only_car_ids_sampled = set(random.sample(only_car_ids, retain_count))\n",
    "print(f\"‚úÖ Number of retained 'only-Car' images after downsampling: {len(only_car_ids_sampled)}\")\n",
    "\n",
    "# Step 4: Filter the image list to exclude dropped-only-Car images\n",
    "filtered_images = []\n",
    "filtered_image_ids = set()\n",
    "for img in coco[\"images\"]:\n",
    "    if img[\"id\"] not in only_car_ids or img[\"id\"] in only_car_ids_sampled:\n",
    "        filtered_images.append(img)\n",
    "        filtered_image_ids.add(img[\"id\"])\n",
    "\n",
    "# Step 5: Filter annotations based on the retained image_ids\n",
    "filtered_annotations = [ann for ann in coco[\"annotations\"] if ann[\"image_id\"] in filtered_image_ids]\n",
    "\n",
    "# Step 6: Create the new COCO-format dictionary\n",
    "filtered_coco = {\n",
    "    \"images\": filtered_images,\n",
    "    \"annotations\": filtered_annotations,\n",
    "    \"categories\": coco[\"categories\"]\n",
    "}\n",
    "\n",
    "# Step 7: Save the balanced annotation file\n",
    "output_path = \"./annotations/auair_coco_train.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(filtered_coco, f, indent=4)\n",
    "\n",
    "print(f\"\\nüéØ Balanced train set saved to: {output_path}\")\n",
    "print(f\"üñºÔ∏è Total remaining images: {len(filtered_images)}\")\n",
    "print(f\"‚úèÔ∏è Total remaining annotations: {len(filtered_annotations)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class AUAirCocoDataset(CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transform=None):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super().__getitem__(idx)\n",
    "        w, h = img.size\n",
    "\n",
    "        boxes = []\n",
    "        class_labels = []\n",
    "\n",
    "        for obj in target:\n",
    "            x, y, width, height = obj[\"bbox\"]\n",
    "            if width > 0 and height > 0:\n",
    "                boxes.append([x, y, x + width, y + height])\n",
    "                class_labels.append(obj[\"category_id\"])\n",
    "\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=img_np,\n",
    "                bboxes=boxes,\n",
    "                category=class_labels\n",
    "            )\n",
    "            img_np = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            class_labels = transformed[\"category\"]\n",
    "\n",
    "        img_tensor = transforms.ToTensor()(img_np)\n",
    "        # image_id = target[0][\"image_id\"]\n",
    "\n",
    "        normalized_boxes = []\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            x1 /= w\n",
    "            x2 /= w\n",
    "            y1 /= h\n",
    "            y2 /= h\n",
    "            normalized_boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(normalized_boxes, dtype=torch.float32),\n",
    "            \"class_labels\": torch.tensor(class_labels, dtype=torch.int64),  # ‚úÖ key name deƒüi≈üti\n",
    "            \"image_id\": torch.tensor([self.ids[idx]]),\n",
    "            \"area\": torch.tensor([(x2 - x1) * (y2 - y1) for x1, y1, x2, y2 in boxes], dtype=torch.float32),\n",
    "            \"iscrowd\": torch.zeros(len(boxes), dtype=torch.int64),\n",
    "            \"orig_size\": torch.tensor([h, w], dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "\n",
    "        return img_tensor, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "# For training\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.HueSaturationValue(p=0.5),\n",
    "        A.Rotate(limit=10, p=0.5),\n",
    "        A.RandomScale(scale_limit=0.2, p=0.5),\n",
    "        A.GaussianBlur(p=0.5),\n",
    "        A.GaussNoise(p=0.5),\n",
    "        A.Resize(500, 500),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"category\"])\n",
    ")\n",
    "\n",
    "# For validation/test (no augmentation)\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(500, 500),\n",
    "\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"category\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AUAirCocoDataset(\n",
    "    img_folder=\"./AU-AIR/images\",\n",
    "    ann_file=\"./annotations/auair_coco_train.json\",\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = AUAirCocoDataset(\n",
    "    img_folder=\"./AU-AIR/images\",\n",
    "    ann_file=\"./annotations/auair_coco_val.json\",\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "test_dataset = AUAirCocoDataset(\n",
    "    img_folder=\"./AU-AIR/images\",\n",
    "    ann_file=\"./annotations/auair_coco_test.json\",\n",
    "    transform=val_transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch] \n",
    "    labels = [item[1] for item in batch]\n",
    "    return images, labels\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Step200_val_loss</td><td>‚ñÅ</td></tr><tr><td>learning_rate</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ</td></tr><tr><td>train_loss_50avg</td><td>‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Step200_val_loss</td><td>2.38061</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>train_loss</td><td>3.09872</td></tr><tr><td>train_loss_50avg</td><td>2.47513</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">detr-resnet-50-dc5-auair</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune/runs/ycvpewur' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune/runs/ycvpewur</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250414_030739-ycvpewur\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg2_repo2\\DI725_Assignment2\\wandb\\run-20250414_032059-5380j43x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune/runs/5380j43x' target=\"_blank\">detr-resnet-50-auair-finetuned</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune/runs/5380j43x' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune/runs/5380j43x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune/runs/5380j43x?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x288ab49d5d0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project=\"DI725_Assignment2_FineTune\",\n",
    "    name=\"detr-resnet-50-auair-finetuned\",\n",
    "    config={\n",
    "        \"model\": \"facebook/detr-resnet-50-dc5\",\n",
    "        \"batch_size\": 4,\n",
    "        \"eval_steps\": 50,\n",
    "        \"max_steps\": 10000,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"weight_decay\": 1e-4\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">detr-resnet-50-auair-finetuned</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune/runs/5380j43x' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune/runs/5380j43x</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250414_032059-5380j43x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg2_repo2\\DI725_Assignment2\\wandb\\run-20250414_032103-2gmx3u4o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune/runs/2gmx3u4o' target=\"_blank\">detr-resnet-50-dc5-auair</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune/runs/2gmx3u4o' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725_Assignment2_FineTune/runs/2gmx3u4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50-dc5 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50-dc5 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([9, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DetrForObjectDetection(\n",
       "  (model): DetrModel(\n",
       "    (backbone): DetrConvModel(\n",
       "      (conv_encoder): DetrConvEncoder(\n",
       "        (model): FeatureListNet(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): DetrFrozenBatchNorm2d()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): DetrSinePositionEmbedding()\n",
       "    )\n",
       "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (query_position_embeddings): Embedding(100, 256)\n",
       "    (encoder): DetrEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrEncoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): DetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrDecoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_labels_classifier): Linear(in_features=256, out_features=9, bias=True)\n",
       "  (bbox_predictor): DetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "wandb.init(\n",
    "    project=\"DI725_Assignment2_FineTune\",\n",
    "    name=\"detr-resnet-50-dc5-auair\",\n",
    ")\n",
    "\n",
    "checkpoint = \"facebook/detr-resnet-50-dc5\"\n",
    "id2label = {\n",
    "    0: \"Human\", 1: \"Car\", 2: \"Truck\", 3: \"Van\",\n",
    "    4: \"Motorbike\", 5: \"Bicycle\", 6: \"Bus\", 7: \"Trailer\"\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 7. Helper Functions\n",
    "# ================================\n",
    "def denormalize_boxes(boxes, width, height):\n",
    "    boxes[:, [0, 2]] *= width\n",
    "    boxes[:, [1, 3]] *= height\n",
    "    return boxes\n",
    "\n",
    "def compute_map(preds, labels):\n",
    "    metric = MeanAveragePrecision(box_format=\"xywh\", class_metrics=True)\n",
    "    metric.update(preds=preds, target=labels)\n",
    "    return metric.compute()\n",
    "\n",
    "def log_map_table(metrics, split=\"Validation\"):\n",
    "    table = wandb.Table(columns=[\n",
    "        \"Model\", \"Human\", \"Car\", \"Truck\", \"Van\",\n",
    "        \"M.bike\", \"Bicycle\", \"Bus\", \"Trailer\", \"mAP\"\n",
    "    ])\n",
    "    row = [\n",
    "        \"DETR-ResNet50\",\n",
    "        round(metrics.get(\"map_Human\", torch.tensor(0.0)).item() * 100, 2),\n",
    "        round(metrics.get(\"map_Car\", torch.tensor(0.0)).item() * 100, 2),\n",
    "        round(metrics.get(\"map_Truck\", torch.tensor(0.0)).item() * 100, 2),\n",
    "        round(metrics.get(\"map_Van\", torch.tensor(0.0)).item() * 100, 2),\n",
    "        round(metrics.get(\"map_Motorbike\", torch.tensor(0.0)).item() * 100, 2),\n",
    "        round(metrics.get(\"map_Bicycle\", torch.tensor(0.0)).item() * 100, 2),\n",
    "        round(metrics.get(\"map_Bus\", torch.tensor(0.0)).item() * 100, 2),\n",
    "        round(metrics.get(\"map_Trailer\", torch.tensor(0.0)).item() * 100, 2),\n",
    "        round(metrics.get(\"map\", torch.tensor(0.0)).item() * 100, 2)\n",
    "    ]\n",
    "    table.add_data(*row)\n",
    "    wandb.log({f\"{split}_mAP_Table\": table})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, device, epochs=5, test_loader=None):\n",
    "    global_step = 0\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.5) \n",
    "\n",
    "    def run_interim_validation(step_tag=\"Interim\"):\n",
    "        model.eval()\n",
    "        val_loss, preds, gts = 0, [], []\n",
    "        val_pbar = tqdm(val_loader, desc=f\"{step_tag} Validation\", leave=True)\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_pbar:\n",
    "                images = torch.stack([img.to(device) for img in images])\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                outputs = model(pixel_values=images, labels=targets)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "                logits = outputs.logits\n",
    "                pred_scores = logits.softmax(-1)[..., :-1]\n",
    "                pred_boxes = outputs.pred_boxes\n",
    "\n",
    "                for i in range(len(images)):\n",
    "                    width, height = targets[i][\"orig_size\"]\n",
    "                    boxes = denormalize_boxes(pred_boxes[i].detach().clone(), width, height)\n",
    "                    labels = pred_scores[i].argmax(-1)\n",
    "                    scores = pred_scores[i].max(-1).values\n",
    "                    print(f\"DEBUG Sample pred_labels: {labels}\")\n",
    "\n",
    "\n",
    "                    preds.append({\"boxes\": boxes, \"scores\": scores, \"labels\": labels})\n",
    "                    print(f\"DEBUG Target[{i}] class_labels:\", targets[i][\"class_labels\"])\n",
    "                    gts.append({\n",
    "                        \"boxes\": denormalize_boxes(targets[i][\"boxes\"].detach().clone(), width, height),\n",
    "                        \"labels\": targets[i][\"class_labels\"].clone()\n",
    "                    })\n",
    "        print(f\"‚úÖ {step_tag} validation done at step {global_step}\")\n",
    "        wandb.log({f\"{step_tag}_val_loss\": val_loss / len(val_loader)}, step=global_step)\n",
    "        metrics = compute_map(preds, gts)\n",
    "        log_map_table(metrics, split=f\"{step_tag} Validation\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, batch_losses = 0, []\n",
    "\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
    "        for i, (images, targets) in pbar:\n",
    "            images = torch.stack([img.to(device) for img in images])\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            outputs = model(pixel_values=images, labels=targets)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            batch_losses.append(loss.item())\n",
    "            wandb.log({\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "            }, step=global_step)\n",
    "\n",
    "            if global_step > 0 and global_step % 50 == 0:\n",
    "                avg_50 = sum(batch_losses[-50:]) / 50\n",
    "                wandb.log({\"train_loss_50avg\": avg_50}, step=global_step)\n",
    "\n",
    "            if global_step > 0 and global_step % 200 == 0:\n",
    "                run_interim_validation(step_tag=f\"Step{global_step}\")\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"batch_loss\": loss.item(),\n",
    "                \"avg_loss\": running_loss / (i + 1),\n",
    "                \"lr\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "            global_step += 1\n",
    "\n",
    "        scheduler.step()  # Update LR at the edn of Epoch\n",
    "        wandb.log({\"train_loss_epoch_avg\": running_loss / len(train_loader)}, step=global_step)\n",
    "\n",
    "        # ===== End of Epoch =====\n",
    "        run_interim_validation(step_tag=f\"Epoch{epoch+1}\")\n",
    "\n",
    "    # ========== TEST EVALUATION ==========\n",
    "    if test_loader:\n",
    "        model.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            test_pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "            for images, targets in test_pbar:\n",
    "                images = torch.stack([img.to(device) for img in images])\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                outputs = model(pixel_values=images, labels=targets)\n",
    "                logits = outputs.logits\n",
    "                pred_scores = logits.softmax(-1)[..., :-1]\n",
    "                pred_boxes = outputs.pred_boxes\n",
    "\n",
    "                for i in range(len(images)):\n",
    "                    width, height = targets[i][\"orig_size\"]\n",
    "                    boxes = denormalize_boxes(pred_boxes[i].detach().clone(), width, height)\n",
    "                    labels = pred_scores[i].argmax(-1)\n",
    "                    scores = pred_scores[i].max(-1).values\n",
    "\n",
    "                    preds.append({\"boxes\": boxes, \"scores\": scores, \"labels\": labels})\n",
    "                    print(f\"DEBUG Target[{i}] class_labels:\", targets[i][\"class_labels\"])\n",
    "                    gts.append({\n",
    "                        \"boxes\": denormalize_boxes(targets[i][\"boxes\"].detach().clone(), width, height),\n",
    "                        \"labels\": targets[i][\"class_labels\"].clone()\n",
    "                    })\n",
    "\n",
    "        test_metrics = compute_map(preds, gts)\n",
    "        log_map_table(test_metrics, split=\"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6:   2%|‚ñè         | 95/4249 [00:56<40:49,  1.70it/s, batch_loss=4.02, avg_loss=4.67, lr=1e-5] "
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 9. Run Training\n",
    "# ================================\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "train(model, train_loader, val_loader, optimizer, device=\"cuda\", epochs=6, test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 10. Final Test Evaluation\n",
    "# ================================\n",
    "model.eval()\n",
    "test_preds, test_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images = [img.to(\"cuda\") for img in images]\n",
    "        targets = [{k: v.to(\"cuda\") for k, v in t.items()} for t in targets]\n",
    "\n",
    "        outputs = model(images)\n",
    "        logits = outputs.logits\n",
    "        pred_scores = logits.softmax(-1)[..., :-1]\n",
    "        pred_boxes = outputs.pred_boxes\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            width, height = targets[i][\"orig_size\"]\n",
    "            boxes = denormalize_boxes(pred_boxes[i].detach().clone(), width, height)\n",
    "            labels = pred_scores[i].argmax(-1)\n",
    "            scores = pred_scores[i].max(-1).values\n",
    "\n",
    "            test_preds.append({\"boxes\": boxes, \"scores\": scores, \"labels\": labels})\n",
    "            test_targets.append({\n",
    "                \"boxes\": denormalize_boxes(targets[i][\"boxes\"].detach().clone(), width, height),\n",
    "                \"labels\": targets[i][\"labels\"]\n",
    "            })\n",
    "\n",
    "test_metrics = compute_map(test_preds, test_targets)\n",
    "log_map_table(test_metrics, split=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, device, epochs=5):\n",
    "    global_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        batch_losses = []\n",
    "\n",
    "        for i, (images, targets) in enumerate(train_loader):\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss_dict = model.compute_loss(outputs, targets)\n",
    "            loss = sum(loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            batch_losses.append(loss.item())\n",
    "            wandb.log({\"train_loss\": loss.item()}, step=global_step)\n",
    "\n",
    "            if global_step > 0 and global_step % 50 == 0:\n",
    "                avg_50 = sum(batch_losses[-50:]) / 50\n",
    "                wandb.log({\"train_loss_50avg\": avg_50}, step=global_step)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        wandb.log({\"train_loss_epoch_avg\": running_loss / len(train_loader)}, step=global_step)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = list(img.to(device) for img in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss_dict = model.compute_loss(outputs, targets)\n",
    "                loss = sum(loss_dict.values())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # For mAP calculation\n",
    "                scores = outputs.logits.softmax(-1)[..., :-1]\n",
    "                boxes = model.post_process(outputs, target_sizes=[t[\"orig_size\"] for t in targets])\n",
    "                preds.extend(boxes)\n",
    "                gts.extend(targets)\n",
    "\n",
    "        wandb.log({\"val_loss\": val_loss / len(val_loader)}, step=global_step)\n",
    "\n",
    "        # Compute and log mAP\n",
    "        metrics = compute_map(preds, gts)\n",
    "        log_map_table(metrics, split=\"Validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb setup (outside this cell)\n",
    "# wandb.init(project=\"DI725_Assignment2_FineTune\", name=\"detr-resnet-50-dc5-auair\")\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "# train(model, train_loader, val_loader, optimizer, device=\"cuda\", epochs=10)\n",
    "# metrics = compute_metrics((None, val_labels), compute_result=True)\n",
    "# log_map_table(metrics, \"Validation\")\n",
    "# metrics = compute_metrics((None, test_labels), compute_result=True)\n",
    "# log_map_table(metrics, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"facebook/detr-resnet-50-dc5\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def load_coco_as_hf_dataset(coco_json_path, image_dir):\n",
    "    with open(coco_json_path, \"r\") as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    images_dict = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "    annotations_by_image = {}\n",
    "\n",
    "    for ann in coco[\"annotations\"]:\n",
    "        img_id = ann[\"image_id\"]\n",
    "        if img_id not in annotations_by_image:\n",
    "            annotations_by_image[img_id] = {\n",
    "                \"bbox\": [],\n",
    "                \"area\": [],\n",
    "                \"category\": [],\n",
    "                \"bbox_id\": []\n",
    "            }\n",
    "\n",
    "        annotations_by_image[img_id][\"bbox\"].append([\n",
    "            ann[\"bbox\"][0],\n",
    "            ann[\"bbox\"][1],\n",
    "            ann[\"bbox\"][0] + ann[\"bbox\"][2],\n",
    "            ann[\"bbox\"][1] + ann[\"bbox\"][3]\n",
    "        ])\n",
    "        annotations_by_image[img_id][\"area\"].append(ann[\"area\"])\n",
    "        annotations_by_image[img_id][\"category\"].append(ann[\"category_id\"])\n",
    "        annotations_by_image[img_id][\"bbox_id\"].append(ann[\"id\"])\n",
    "\n",
    "    hf_data = []\n",
    "    for img_id, anns in annotations_by_image.items():\n",
    "        img = images_dict[img_id]\n",
    "        hf_data.append({\n",
    "            \"image_id\": img[\"id\"],\n",
    "            \"file_name\": f\"{image_dir}/{img['file_name']}\",\n",
    "            \"width\": img[\"width\"],\n",
    "            \"height\": img[\"height\"],\n",
    "            \"objects\": anns\n",
    "        })\n",
    "\n",
    "    return Dataset.from_list(hf_data)\n",
    "\n",
    "# √ñrnek kullanƒ±m\n",
    "train_dataset = load_coco_as_hf_dataset(\"./annotations/auair_coco_train.json\", \"./AU-AIR/images\")\n",
    "val_dataset = load_coco_as_hf_dataset(\"./annotations/auair_coco_val.json\", \"./AU-AIR/images\")\n",
    "test_dataset = load_coco_as_hf_dataset(\"./annotations/auair_coco_test.json\", \"./AU-AIR/images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def convert_voc_to_coco(bbox):\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    return [x_min, y_min, x_max - x_min, y_max - y_min]\n",
    "\n",
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    return [{\n",
    "        \"image_id\": image_id,\n",
    "        \"category_id\": category[i],\n",
    "        \"isCrowd\": 0,\n",
    "        \"area\": area[i],\n",
    "        \"bbox\": list(bbox[i])\n",
    "    } for i in range(len(category))]\n",
    "\n",
    "def transform_aug_ann(example, transform):\n",
    "\n",
    "    # 1Ô∏è‚É£ Load image from file path\n",
    "    file_path = example[\"file_name\"]\n",
    "    if isinstance(file_path, list):\n",
    "        file_path = file_path[0]\n",
    "\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    image_np = np.array(image)[:, :, ::-1]\n",
    "\n",
    "    # 2Ô∏è‚É£ Merge all objects (flatten bbox, category, area)\n",
    "    merged_bboxes = []\n",
    "    merged_categories = []\n",
    "    merged_areas = []\n",
    "\n",
    "    for obj in example[\"objects\"]:\n",
    "        merged_bboxes.extend(obj[\"bbox\"])\n",
    "        merged_categories.extend(obj[\"category\"])\n",
    "        merged_areas.extend(obj[\"area\"])\n",
    "\n",
    "    # 3Ô∏è‚É£ Apply Albumentations\n",
    "    augmented = transform(\n",
    "        image=image_np,\n",
    "        bboxes=merged_bboxes,\n",
    "        category=merged_categories\n",
    "    )\n",
    "\n",
    "    boxes = [convert_voc_to_coco(b) for b in augmented[\"bboxes\"]]\n",
    "    categories = augmented[\"category\"]\n",
    "    areas = merged_areas[:len(categories)]  # ‚ö†Ô∏è e≈üle≈ümeyen olursa kes\n",
    "\n",
    "    # 4Ô∏è‚É£ Format for image processor\n",
    "    annotations = formatted_anns(example[\"image_id\"], categories, areas, boxes)\n",
    "\n",
    "    processed = image_processor(\n",
    "        images=[augmented[\"image\"]],\n",
    "        annotations=[{\"image_id\": example[\"image_id\"], \"annotations\": annotations}],\n",
    "        # return_tensors=\"pt\"\n",
    "        return_tensors = None\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        # \"pixel_values\": processed[\"pixel_values\"].squeeze(0),\n",
    "        \"pixel_values\": processed[\"pixel_values\"][0],\n",
    "        \"labels\": processed[\"labels\"][0]\n",
    "    }\n",
    "\n",
    "train_dataset = train_dataset.with_transform(lambda ex: transform_aug_ann(ex, train_transform))\n",
    "val_dataset = val_dataset.with_transform(lambda ex: transform_aug_ann(ex, val_transform))\n",
    "test_dataset = test_dataset.with_transform(lambda ex: transform_aug_ann(ex, val_transform))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.0.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.0.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.0.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.0.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.0.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.0.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.0.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.0.downsample.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.0.downsample.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.0.downsample.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.1.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.1.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.1.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.1.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.1.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.1.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.1.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.2.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.2.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.2.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.2.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.2.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.2.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.2.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.2.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer1.2.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.0.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.0.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.0.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.0.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.0.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.0.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.0.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.0.downsample.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.0.downsample.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.0.downsample.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.1.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.1.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.1.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.1.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.1.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.1.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.1.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.2.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.2.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.2.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.2.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.2.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.2.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.2.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.2.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.2.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.3.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.3.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.3.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.3.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.3.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.3.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.3.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.3.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer2.3.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.0.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.0.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.0.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.0.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.0.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.0.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.0.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.0.downsample.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.0.downsample.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.0.downsample.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.1.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.1.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.1.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.1.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.1.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.1.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.1.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.2.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.2.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.2.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.2.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.2.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.2.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.2.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.2.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.2.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.3.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.3.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.3.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.3.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.3.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.3.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.3.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.3.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.3.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.4.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.4.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.4.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.4.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.4.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.4.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.4.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.4.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.4.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.5.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.5.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.5.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.5.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.5.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.5.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.5.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.5.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer3.5.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.0.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.0.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.0.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.0.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.0.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.0.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.0.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.0.downsample.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.0.downsample.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.0.downsample.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.1.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.1.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.1.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.1.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.1.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.1.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.1.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.2.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.2.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.2.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.2.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.2.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.2.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.2.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.2.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2400: UserWarning: for layer4.2.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50-dc5 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50-dc5 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([9, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "output_dir = \"detr-resnet-50-dc5-auair-finetuned\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (8.1.6)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (8.34.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets) (3.0.14)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: decorator in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (4.13.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38b994242dc462880b158b554126c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "import torch\n",
    "\n",
    "# Define the training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    max_steps=10000,\n",
    "    fp16=True,\n",
    "    save_steps=10,\n",
    "    logging_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    eval_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=True,\n",
    "    batch_eval_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">detr-resnet-50-dc5-auair-finetuned</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assigment_2_FineTune/runs/p5jyjd8u' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725_Assigment_2_FineTune/runs/p5jyjd8u</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assigment_2_FineTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725_Assigment_2_FineTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250413_233608-p5jyjd8u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg2_repo2\\DI725_Assignment2\\wandb\\run-20250413_234207-ooqnzt6n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725_Assigment_2_FineTune/runs/ooqnzt6n' target=\"_blank\">detr-resnet-50-dc5-auair-finetuned</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assigment_2_FineTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assigment_2_FineTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725_Assigment_2_FineTune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725_Assigment_2_FineTune/runs/ooqnzt6n' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725_Assigment_2_FineTune/runs/ooqnzt6n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aliyigitbasaran-/DI725_Assigment_2_FineTune/runs/ooqnzt6n?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x26ba855bc40>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"DI725_Assigment_2_FineTune\",  # change this\n",
    "    name=\"detr-resnet-50-dc5-auair-finetuned\",  # change this\n",
    "    config=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "def denormalize_boxes(boxes, width, height):\n",
    "    boxes = boxes.clone()\n",
    "    boxes[:, 0] *= width  # xmin\n",
    "    boxes[:, 1] *= height  # ymin\n",
    "    boxes[:, 2] *= width  # xmax\n",
    "    boxes[:, 3] *= height  # ymax\n",
    "    return boxes\n",
    "\n",
    "\n",
    "batch_metrics = []\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred, compute_result):\n",
    "    global batch_metrics\n",
    "\n",
    "    (loss_dict, scores, pred_boxes, last_hidden_state, encoder_last_hidden_state), labels = eval_pred\n",
    "\n",
    "    image_sizes = []\n",
    "    target = []\n",
    "    for label in labels:\n",
    "\n",
    "        image_sizes.append(label[\"orig_size\"])\n",
    "        width, height = label[\"orig_size\"]\n",
    "        denormalized_boxes = denormalize_boxes(label[\"boxes\"], width, height)\n",
    "        target.append(\n",
    "            {\n",
    "                \"boxes\": denormalized_boxes,\n",
    "                \"labels\": label[\"class_labels\"],\n",
    "            }\n",
    "        )\n",
    "    predictions = []\n",
    "    for score, box, target_sizes in zip(scores, pred_boxes, image_sizes):\n",
    "        # Extract the bounding boxes, labels, and scores from the model's output\n",
    "        pred_scores = score[:, :-1]  # Exclude the no-object class\n",
    "        pred_scores = softmax(pred_scores, dim=-1)\n",
    "        width, height = target_sizes\n",
    "        pred_boxes = denormalize_boxes(box, width, height)\n",
    "        pred_labels = torch.argmax(pred_scores, dim=-1)\n",
    "\n",
    "        # Get the scores corresponding to the predicted labels\n",
    "        pred_scores_for_labels = torch.gather(pred_scores, 1, pred_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        predictions.append(\n",
    "            {\n",
    "                \"boxes\": pred_boxes,\n",
    "                \"scores\": pred_scores_for_labels,\n",
    "                \"labels\": pred_labels,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    metric = MeanAveragePrecision(box_format=\"xywh\", class_metrics=True)\n",
    "\n",
    "    if not compute_result:\n",
    "        # Accumulate batch-level metrics\n",
    "        batch_metrics.append({\"preds\": predictions, \"target\": target})\n",
    "        return {}\n",
    "    else:\n",
    "        # Compute final aggregated metrics\n",
    "        # Aggregate batch-level metrics (this should be done based on your metric library's needs)\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        for batch in batch_metrics:\n",
    "            all_preds.extend(batch[\"preds\"])\n",
    "            all_targets.extend(batch[\"target\"])\n",
    "\n",
    "        # Update metric with all accumulated predictions and targets\n",
    "        metric.update(preds=all_preds, target=all_targets)\n",
    "        metrics = metric.compute()\n",
    "\n",
    "        # Convert and format metrics as needed\n",
    "        classes = metrics.pop(\"classes\")\n",
    "        map_per_class = metrics.pop(\"map_per_class\")\n",
    "        mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
    "\n",
    "        for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
    "            class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
    "            metrics[f\"map_{class_name}\"] = class_map\n",
    "            metrics[f\"mar_100_{class_name}\"] = class_mar\n",
    "\n",
    "        # Round metrics for cleaner output\n",
    "        metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
    "\n",
    "        # Clear batch metrics for next evaluation\n",
    "        batch_metrics = []\n",
    "\n",
    "        return metrics\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_23096\\3631066672.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 1, 'file_name': './AU-AIR/images/frame_20190829091111_x_0001973.jpg', 'width': 1920.0, 'height': 1080.0, 'objects': {'area': [77700, 69168, 27999], 'bbox': [[1098, 163, 1518, 348], [1128, 421, 1521, 597], [1703, 927, 1886, 1080]], 'bbox_id': [1, 2, 3], 'category': [1, 1, 0]}}\n"
     ]
    }
   ],
   "source": [
    "train_dataset.set_format(type=None, columns=None)\n",
    "val_dataset.set_format(type=None, columns=None)\n",
    "test_dataset.set_format(type=None, columns=None)\n",
    "\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.with_transform(lambda ex: transform_aug_ann(ex, train_transform))\n",
    "val_dataset = val_dataset.with_transform(lambda ex: transform_aug_ann(ex, val_transform))\n",
    "test_dataset = test_dataset.with_transform(lambda ex: transform_aug_ann(ex, val_transform))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Indexing with integers is not available when using Python based feature extractors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[266], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:2783\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[1;34m(self, keys)\u001b[0m\n\u001b[0;32m   2781\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(keys)\n\u001b[0;32m   2782\u001b[0m n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m-> 2783\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:2783\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2781\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(keys)\n\u001b[0;32m   2782\u001b[0m n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m-> 2783\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:2783\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2781\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(keys)\n\u001b[0;32m   2782\u001b[0m n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m-> 2783\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [{col: \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\feature_extraction_utils.py:88\u001b[0m, in \u001b[0;36mBatchFeature.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing with integers is not available when using Python based feature extractors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Indexing with integers is not available when using Python based feature extractors'"
     ]
    }
   ],
   "source": [
    "batch = next(iter(DataLoader(train_dataset, batch_size=1, collate_fn=collate_fn)))\n",
    "print(batch[\"pixel_values\"].shape)\n",
    "print(batch[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pixel_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[258], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2236\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[1;32m-> 2236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2237\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2243\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2514\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2512\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2513\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2514\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2516\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:5243\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[0;32m   5241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5243\u001b[0m         batch_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   5244\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5245\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\data_loader.py:566\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 566\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[254], line 92\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollate_fn\u001b[39m(batch):\n\u001b[1;32m---> 92\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     93\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m image_processor\u001b[38;5;241m.\u001b[39mpad(pixel_values, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n",
      "Cell \u001b[1;32mIn[254], line 92\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollate_fn\u001b[39m(batch):\n\u001b[1;32m---> 92\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m [\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpixel_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     93\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m image_processor\u001b[38;5;241m.\u001b[39mpad(pixel_values, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pixel_values'"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMTnwacVdhftT5KzXw+jI8b",
   "gpuType": "T4",
   "mount_file_id": "1x8MuI6tK7UNAYP0VPo4DSFnKw8lbeoBM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
